[
{"contents": "We are excited to announce our newest data extraction API. The  is now out of BETA and publicly available as a stable release.\u00a0If you are ready to roll up your sleeves and get started, here are the links you need:While this blog covers most of the notable improvements & extensive testing that the API has undergone, that warrants an exit from Beta, together with some high-level uses; it\u2019s important to remember that we have already covered it "},
{"contents": "Generally, there are 3 steps needed to find the best proxy management method for your web scraping project and to make sure you can get data not just today but also in the future, long-term.You need to define the traffic profile first to determine the concrete needs of your project. What is a traffic profile?It includes, first of all, the  that you're trying to get data from. And also if there's any technical challenges needed to be solved, like JS rendering."},
{"contents": "Today we are delighted to launch a Beta of our newest data extraction API: . With this API you can collect structured data from web pages that contain automotive data such as classified or dealership sites. Using our API, you can get your data without writing site-specific code. If you need automotive/vehicle data, sign up now for a beta version of our Vehicle API.Whether you are interested in car prices, VIN or other car specific details, our Vehicle API can extract that data for you, at scale.With "},
{"contents": "When you extract data from the web at scale, quality assurance is an important process to make sure your web extracted data is consistently of high quality. Validation of this data can be complex though. There are many challenges and problems that need to be addressed. In the second part of this series on web data quality assurance, we will cover most common hurdles and pitfalls in data validation and how to deal with them.I still remember my first lesson when I joined the team. My manager shared 3 simple questions to keep in our mind working on data validation:The problems will be listed in their natural appearance in a typical web scraping project.In the previous post, we discussed the importance of clear, testable requirements. Now let's add more details about what else could be challenging at this point."},
{"contents": "Web scraping projects usually involve data extraction from many websites. The standard approach to tackle this problem is to write some code to navigate and extract the data from each website. However, this approach may not scale so nicely in the long-term, requiring maintenance effort for each website; it also doesn\u2019t scale in the short-term, when we need to start the extraction process in a couple of weeks. Therefore, we need to think of different solutions to tackle these issues.The problem we propose to solve here is related to  that can be available in HTML form or files, such as PDFs. The catch is that this is required for a few hundreds of different domains and we should be able to scale it up and down without much effort.A brief outline of the problem that needs to be solved:"},
{"contents": "We are excited to announce our next . Using this API, you can get access to product reviews in a structured format, without writing site-specific code. You can use the Product Reviews API to extract product reviews from eCommerce sites at scale. Just make a request to the API and receive your data in real-time!In today\u2019s competitive eCommerce world, product reviews provide a great way for online shoppers to determine what products to buy. Hence, monitoring product reviews are important for businesses. Making use of reviews data, you can find insights in the data that can improve your decision making, address feedback, and monitor customer sentiment.But getting access to structured web data is not easy, especially if you don\u2019t have the right tools. With Product Reviews API, we provide a convenient way for you to extract reviews at scale from any site."},
{"contents": "The manual way or the highway...In software testing and QA circles, the topic of whether automated or manual testing is superior remains a hotly debated one. For  QA and validation specifically, they are not mutually exclusive. Indeed, for data, manual QA can inform automated QA, and vice versa. In this post, we\u2019ll give some examples.It is rare that "},
{"contents": "As the COVID-19 pandemic took hold, we at Scrapinghub began to wonder how it would impact on the data we crawl, and whether that data could tell us something useful about the pandemic and its impact.Retail price intelligence is one of our key areas of interest. On a daily basis,  for price and stock level information. What insights might be hidden in that data?To explore this, we identified a basket of goods related to pandemic preparedness (eg Face Masks, medications, etc) and began to track the number of individual items available online in this set, and the average item price over time."},
{"contents": "Article and news data extraction is becoming increasingly popular and widely used by companies. Data quality plays a vital role in making sure these projects succeed. If the quality of the extracted articles is not good enough, your whole business could be at risk, especially if it depends on the constant flow of high quality article data.When it comes to web data extraction, data quality is always a key factor. Without high data quality, organizations face increased costs () let alone having their competitive standing undermined. If you\u2019re looking for an article extraction solution, your top priority should be data quality. You need to know which service or library provides the best article data quality. You need to learn what metrics are important when you "},
{"contents": "The web is complex and constantly changing. It is one of the reasons why web data extraction can be difficult, especially in the long term. It\u2019s necessary to understand how a website works really well, before you try to extract data. Luckily, there are lots of inspection and code tools available for this and in this article we will show you some of our favorites.All major browsers come packed with a set of development tools. Although these have been built with the goal of building websites in mind, they can also be used to analyze web pages and traffic. These are some pretty powerful tools for working with websites.For Google Chrome, these developer tools can be accessed from any page by right-clicking then choosing 'Inspect' or using the shortcut 'CTRL + shift + I' (or '\u2318 + Option + I' on macs).You can use these tools to perform some basic operations:"}
]